"""
å‘é‡åŒ–æœåŠ¡æ¨¡å— - è´Ÿè´£ç”Ÿæˆå’Œç®¡ç†è¡£ç‰©çš„è¯­ä¹‰å‘é‡ï¼ˆå¤šæ¨¡æ€ï¼šæ–‡æœ¬+å›¾åƒï¼‰
"""
import os
from typing import List, Dict, Any, Optional
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import threading
import torch
from PIL import Image
from transformers import CLIPModel, CLIPProcessor
import numpy as np


class EmbeddingService:
    """è¡£æ©±å‘é‡åŒ–æœåŠ¡ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        """å»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…é‡å¤åŠ è½½"""
        if self._initialized:
            return
            
        print("ğŸ”§ åˆå§‹åŒ–å‘é‡åŒ–æœåŠ¡...")
        
        # è®¾ç½®å®Œå…¨ç¦»çº¿æ¨¡å¼ï¼ˆåœ¨åŠ è½½ä»»ä½•æ¨¡å‹ä¹‹å‰ï¼‰
        import os
        from pathlib import Path
        
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
        os.environ['TRANSFORMERS_OFFLINE'] = '1'  # å¼ºåˆ¶ç¦»çº¿
        os.environ['HF_HUB_OFFLINE'] = '1'  # ç¦ç”¨hubæ£€æŸ¥
        os.environ['HF_DATASETS_OFFLINE'] = '1'  # ç¦ç”¨datasetsæ£€æŸ¥
        os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(Path.home() / ".cache" / "huggingface")
        
        # 1. åŠ è½½æ–‡æœ¬Embeddingæ¨¡å‹ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
        text_model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
        print(f"ğŸ“¥ åŠ è½½æ–‡æœ¬Embeddingæ¨¡å‹: {text_model_name}")
        print(f"ğŸ’¡ å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ï¼Œä»ç¼“å­˜åŠ è½½: ~/.cache/huggingface/")
        
        try:
            # å…ˆå°è¯•ä»æœ¬åœ°ç¼“å­˜ç›®å½•ç›´æ¥åŠ è½½ï¼ˆå®Œå…¨ç¦»çº¿ï¼‰
            cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
            text_cache_path = cache_dir / "models--sentence-transformers--paraphrase-multilingual-mpnet-base-v2" / "snapshots"
            
            if text_cache_path.exists():
                # æŸ¥æ‰¾æœ€æ–°çš„snapshotç›®å½•
                snapshot_dirs = [d for d in text_cache_path.iterdir() if d.is_dir()]
                if snapshot_dirs:
                    latest_snapshot = max(snapshot_dirs, key=lambda p: p.stat().st_mtime)
                    print(f"ğŸ’¾ ä½¿ç”¨æœ¬åœ°ç¼“å­˜: {latest_snapshot}")
                    
                    # æ–¹æ³•ï¼šä»snapshotåŠ è½½ï¼Œä½†æ‰‹åŠ¨åŠ è½½poolingé…ç½®
                    # 1) å…ˆåŠ è½½åŸºç¡€æ¨¡å‹
                    from sentence_transformers import models
                    word_embedding_model = models.Transformer(str(latest_snapshot))
                    
                    # 2) åŠ è½½poolingé…ç½®
                    pooling_config_path = latest_snapshot / "1_Pooling" / "config.json"
                    if pooling_config_path.exists():
                        import json
                        with open(pooling_config_path) as f:
                            pooling_config = json.load(f)
                        # æ­£ç¡®è§£æpoolingå‚æ•°ï¼ˆconfigä¸­æ˜¯å¸ƒå°”å€¼å­—æ®µï¼‰
                        pooling_model = models.Pooling(
                            word_embedding_model.get_word_embedding_dimension(),
                            pooling_mode_mean_tokens=pooling_config.get('pooling_mode_mean_tokens', True),
                            pooling_mode_cls_token=pooling_config.get('pooling_mode_cls_token', False),
                            pooling_mode_max_tokens=pooling_config.get('pooling_mode_max_tokens', False)
                        )
                        print(f"âœ… Poolingé…ç½®åŠ è½½æˆåŠŸ: mean={pooling_config.get('pooling_mode_mean_tokens')}, cls={pooling_config.get('pooling_mode_cls_token')}")
                    else:
                        # æ²¡æœ‰poolingé…ç½®ï¼Œä½¿ç”¨é»˜è®¤mean pooling
                        pooling_model = models.Pooling(
                            word_embedding_model.get_word_embedding_dimension(),
                            pooling_mode_mean_tokens=True
                        )
                        print(f"âš ï¸  ä½¿ç”¨é»˜è®¤mean pooling")
                    
                    # 3) ç»„è£…å®Œæ•´æ¨¡å‹
                    self.text_encoder = SentenceTransformer(modules=[word_embedding_model, pooling_model])
                    self.text_dim = self.text_encoder.get_sentence_embedding_dimension()
                    print(f"âœ… æ–‡æœ¬æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‘é‡ç»´åº¦: {self.text_dim}")
                else:
                    raise FileNotFoundError("æœªæ‰¾åˆ°snapshotç›®å½•")
            else:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°ç¼“å­˜ç›®å½•: {text_cache_path}")
            
            # 2. åŠ è½½CLIPå›¾åƒæ¨¡å‹
            clip_model_name = "openai/clip-vit-base-patch32"
            print(f"ğŸ“¥ åŠ è½½CLIPå›¾åƒæ¨¡å‹: {clip_model_name}")
            
            clip_cache_path = cache_dir / "models--openai--clip-vit-base-patch32" / "snapshots"
            if clip_cache_path.exists():
                snapshot_dirs = [d for d in clip_cache_path.iterdir() if d.is_dir()]
                if snapshot_dirs:
                    latest_clip = max(snapshot_dirs, key=lambda p: p.stat().st_mtime)
                    print(f"ğŸ’¾ ä½¿ç”¨CLIPæœ¬åœ°ç¼“å­˜: {latest_clip}")
                    # ä½¿ç”¨safetensorsæ ¼å¼ï¼Œé¿å…PyTorchç‰ˆæœ¬é™åˆ¶
                    self.clip_model = CLIPModel.from_pretrained(str(latest_clip), local_files_only=True, use_safetensors=True)
                    self.clip_processor = CLIPProcessor.from_pretrained(str(latest_clip), local_files_only=True)
                else:
                    raise FileNotFoundError("CLIPç¼“å­˜ç›®å½•å­˜åœ¨ä½†æ— snapshot")
            else:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°CLIPç¼“å­˜: {clip_cache_path}")
            
            self.clip_model.eval()  # æ¨ç†æ¨¡å¼
            # CLIP get_image_featureså®é™…è¾“å‡ºvision_modelçš„hidden_sizeï¼Œä¸æ˜¯projection_dim
            self.image_dim = self.clip_model.config.vision_config.hidden_size  # 768ç»´
            print(f"âœ… CLIPæ¨¡å‹åŠ è½½æˆåŠŸï¼Œå›¾åƒå‘é‡ç»´åº¦: {self.image_dim}")
            
            # è®¡ç®—èåˆå‘é‡ç»´åº¦
            self.total_dim = self.text_dim + self.image_dim  # 768 + 768 = 1536
            print(f"ğŸ”— å¤šæ¨¡æ€èåˆå‘é‡æ€»ç»´åº¦: {self.total_dim}")
            
            self.model_available = True
                
        except Exception as e:
            print(f"âš ï¸  Embeddingæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print(f"ğŸš¨ å‘é‡æ£€ç´¢åŠŸèƒ½å°†è¢«ç¦ç”¨ï¼Œæ¨èå°†ä½¿ç”¨å…¨é‡æŸ¥è¯¢")
            self.text_encoder = None
            self.clip_model = None
            self.clip_processor = None
            self.model_available = False
            # ä¸æŠ›å¼‚å¸¸ï¼Œå…è®¸ç³»ç»Ÿé™çº§è¿è¡Œ
        
        # 2. åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯ï¼ˆä»…å½“æ¨¡å‹å¯ç”¨æ—¶ï¼‰
        if self.model_available:
            chroma_data_path = os.path.join(os.getcwd(), "chroma_data")
            os.makedirs(chroma_data_path, exist_ok=True)
            
            self.chroma_client = chromadb.PersistentClient(
                path=chroma_data_path,
                settings=Settings(anonymized_telemetry=False)
            )
            
            # 3. åˆ›å»ºæˆ–è·å–è¡£æ©±å‘é‡é›†åˆ
            self.wardrobe_collection = self.chroma_client.get_or_create_collection(
                name="wardrobe_items",
                metadata={"description": "ç”¨æˆ·è¡£æ©±è¯­ä¹‰å‘é‡å­˜å‚¨ï¼ˆå¤šæ¨¡æ€1536ç»´ï¼‰"}
            )
            
            print(f"âœ… ChromaDBåˆå§‹åŒ–æˆåŠŸï¼Œæ•°æ®è·¯å¾„: {chroma_data_path}")
            print(f"ğŸ“Š å½“å‰å‘é‡åº“ä¸­å·²æœ‰ {self.wardrobe_collection.count()} æ¡è®°å½•")
        else:
            self.chroma_client = None
            self.wardrobe_collection = None
            print(f"âš ï¸  ChromaDBæœªåˆå§‹åŒ–ï¼Œå‘é‡æ£€ç´¢åŠŸèƒ½ä¸å¯ç”¨")
        
        self._initialized = True
    
    def generate_text_embedding(self, item: Dict[str, Any]) -> List[float]:
        """
        ç”Ÿæˆæ–‡æœ¬è¯­ä¹‰å‘é‡
        """
        if not self.model_available:
            return []  # æ¨¡å‹ä¸å¯ç”¨æ—¶è¿”å›ç©ºå‘é‡
        
        # æ„å»ºè¯­ä¹‰æ–‡æœ¬ï¼ˆä½¿ç”¨è‹±æ–‡å­—æ®µï¼Œæ¨¡å‹å¯¹è‹±æ–‡ç†è§£æ›´å‡†ç¡®ï¼‰
        text_parts = [
            item.get("name_en", item.get("name", "")),
            item.get("color_en", item.get("color", "")),
            item.get("material_en", item.get("material", "")),
            item.get("season", ""),
            item.get("category", "")
        ]
        
        # è¿‡æ»¤ç©ºå€¼ï¼Œæ‹¼æ¥æˆè¯­ä¹‰æ–‡æœ¬
        semantic_text = " ".join([part for part in text_parts if part]).strip()
        
        if not semantic_text:
            print(f"âš ï¸  è­¦å‘Š: è¡£ç‰©ä¿¡æ¯ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å‘é‡")
            semantic_text = "unknown clothing item"
        
        # ç”Ÿæˆå‘é‡ï¼ˆCPUæ¨ç†çº¦50-100msï¼‰
        embedding = self.text_encoder.encode(semantic_text, convert_to_numpy=True)
        return embedding.tolist()
    
    def generate_image_embedding(self, image_path: str) -> List[float]:
        """
        ç”Ÿæˆå›¾åƒè¯­ä¹‰å‘é‡ï¼ˆä½¿ç”¨CLIPï¼‰
        """
        if not self.model_available or not self.clip_model:
            return []  # CLIPæ¨¡å‹ä¸å¯ç”¨æ—¶è¿”å›ç©ºå‘é‡
        
        try:
            # åŠ è½½å›¾åƒ
            image = Image.open(image_path).convert("RGB")
            
            # CLIPé¢„å¤„ç†
            inputs = self.clip_processor(images=image, return_tensors="pt")
            
            # ç”Ÿæˆå›¾åƒå‘é‡ï¼ˆä½¿ç”¨CLIPçš„vision encoderï¼‰
            with torch.no_grad():
                outputs = self.clip_model.get_image_features(**inputs)
            
            # æå–çœŸæ­£çš„tensorï¼ˆoutputsæ˜¯BaseModelOutputWithPoolingå¯¹è±¡ï¼‰
            # ç›´æ¥è®¿é—®åº•å±‚tensoræ•°æ®
            if hasattr(outputs, 'last_hidden_state'):
                image_features = outputs.last_hidden_state[:, 0, :]  # å–CLS token
            elif hasattr(outputs, 'pooler_output'):
                image_features = outputs.pooler_output
            else:
                # å¦‚æœæ˜¯tensorå°±ç›´æ¥ç”¨
                image_features = outputs
            
            # å½’ä¸€åŒ–å‘é‡ï¼ˆç°åœ¨æ˜¯çœŸæ­£çš„tensoräº†ï¼‰
            norm = torch.norm(image_features, p=2, dim=-1, keepdim=True)
            image_features = image_features / norm
            
            return image_features.squeeze().cpu().numpy().tolist()
            
        except Exception as e:
            print(f"âŒ å›¾åƒå‘é‡ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def generate_embedding(self, item: Dict[str, Any], image_path: Optional[str] = None) -> List[float]:
        """
        ç”Ÿæˆå¤šæ¨¡æ€èåˆå‘é‡ï¼ˆæ–‡æœ¬ + å›¾åƒï¼‰
        
        Args:
            item: è¡£ç‰©ä¿¡æ¯å­—å…¸
            image_path: è¡£ç‰©å›¾åƒè·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            èåˆå‘é‡ (768ç»´æ–‡æœ¬ + 768ç»´å›¾åƒ = 1536ç»´)
        """
        if not self.model_available:
            return []  # æ¨¡å‹ä¸å¯ç”¨æ—¶è¿”å›ç©ºå‘é‡
        
        # 1. ç”Ÿæˆæ–‡æœ¬å‘é‡
        text_vec = self.generate_text_embedding(item)
        
        # 2. ç”Ÿæˆå›¾åƒå‘é‡ï¼ˆå¦‚æœæä¾›äº†å›¾åƒè·¯å¾„ï¼‰
        if image_path and os.path.exists(image_path):
            image_vec = self.generate_image_embedding(image_path)
            if not image_vec:
                # å›¾åƒå‘é‡ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é›¶å‘é‡å¡«å……
                image_vec = [0.0] * self.image_dim
        else:
            # æ— å›¾åƒï¼Œä½¿ç”¨é›¶å‘é‡
            image_vec = [0.0] * self.image_dim
        
        # 3. æ‹¼æ¥èåˆå‘é‡
        fused_embedding = text_vec + image_vec
        
        return fused_embedding
    
    def add_item(self, item_id: int, item: Dict[str, Any], image_path: Optional[str] = None) -> bool:
        """å°†è¡£ç‰©å‘é‡æ·»åŠ åˆ°ChromaDB"""
        if not self.model_available:
            return True  # æ¨¡å‹ä¸å¯ç”¨æ—¶é™é»˜è¿”å›æˆåŠŸ
        
        try:
            # ç”Ÿæˆå¤šæ¨¡æ€èåˆå‘é‡
            embedding = self.generate_embedding(item, image_path)
            
            # æ„å»ºå…ƒæ•°æ®ï¼ˆç”¨äºæ··åˆæ£€ç´¢çš„ç²¾ç¡®è¿‡æ»¤ï¼‰
            metadata = {
                "category": item.get("category", "unknown"),
                "season": item.get("season", "all"),
                "color_en": item.get("color_en", ""),
                "material_en": item.get("material_en", ""),  # æ–°å¢ï¼šæè´¨
                "style": item.get("style", ""),  # æ–°å¢ï¼šé£æ ¼
                "user_id": str(item.get("user_id", 0))
            }
            
            # æ·»åŠ åˆ°ChromaDB
            self.wardrobe_collection.add(
                ids=[str(item_id)],
                embeddings=[embedding],
                metadatas=[metadata],
                documents=[item.get("name_en", item.get("name", "Unknown"))]
            )
            
            print(f"âœ… å‘é‡æ·»åŠ æˆåŠŸ: item_id={item_id}, text='{item.get('name_en', '')}', has_image={bool(image_path)}")
            return True
            
        except Exception as e:
            print(f"âŒ å‘é‡æ·»åŠ å¤±è´¥: item_id={item_id}, error={e}")
            return False
    
    def delete_item(self, item_id: int) -> bool:
        """ä»ChromaDBåˆ é™¤è¡£ç‰©å‘é‡"""
        if not self.model_available:
            return True  # æ¨¡å‹ä¸å¯ç”¨æ—¶é™é»˜è¿”å›æˆåŠŸ
            
        try:
            self.wardrobe_collection.delete(ids=[str(item_id)])
            print(f"âœ… å‘é‡åˆ é™¤æˆåŠŸ: item_id={item_id}")
            return True
        except Exception as e:
            print(f"âŒ å‘é‡åˆ é™¤å¤±è´¥: item_id={item_id}, error={e}")
            return False
    
    def search_similar_items(
        self,
        query_text: str = None,
        query_image_path: str = None,
        user_id: int = None,
        top_k: int = 15,
        season_filter: Optional[List[str]] = None,
        category_filter: Optional[str] = None,
        color_filter: Optional[str] = None,  # æ–°å¢ï¼šé¢œè‰²è¿‡æ»¤
        material_filter: Optional[str] = None,  # æ–°å¢ï¼šæè´¨è¿‡æ»¤
        min_score: float = 0.0  # æ–°å¢ï¼šæœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
    ) -> List[int]:
        """
        æ··åˆæ£€ç´¢ç­–ç•¥ï¼šå‘é‡ç›¸ä¼¼åº¦ + ç²¾ç¡®è¿‡æ»¤ + é‡æ’åº
        
        Args:
            query_text: æ–‡æœ¬æŸ¥è¯¢ï¼ˆå¯é€‰ï¼‰
            query_image_path: å›¾åƒæŸ¥è¯¢è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            user_id: ç”¨æˆ·ID
            top_k: è¿”å›ç»“æœæ•°é‡
            season_filter: å­£èŠ‚è¿‡æ»¤
            category_filter: ç±»åˆ«è¿‡æ»¤
            color_filter: é¢œè‰²è¿‡æ»¤ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
            material_filter: æè´¨è¿‡æ»¤ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
            min_score: æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
        """
        if not self.model_available:
            return []  # æ¨¡å‹ä¸å¯ç”¨æ—¶è¿”å›ç©ºåˆ—è¡¨ï¼Œè§¦å‘é™çº§æŸ¥è¯¢
        
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = []
            
            # 1. æ–‡æœ¬å‘é‡
            if query_text:
                text_vec = self.text_encoder.encode(query_text, convert_to_numpy=True).tolist()
            else:
                text_vec = [0.0] * self.text_dim
            
            # 2. å›¾åƒå‘é‡
            if query_image_path and os.path.exists(query_image_path):
                image_vec = self.generate_image_embedding(query_image_path)
                if not image_vec:
                    image_vec = [0.0] * self.image_dim
            else:
                # çº¯æ–‡æœ¬æŸ¥è¯¢ï¼šå›¾åƒç»´åº¦å¡«å……ä¸­é—´å€¼ï¼Œé™ä½å¯¹ç›¸ä¼¼åº¦çš„å½±å“
                # å‡è®¾å›¾åƒå‘é‡çš„å¹³å‡å€¼çº¦ä¸º0ï¼ˆç»è¿‡L2å½’ä¸€åŒ–ï¼‰ï¼Œä½¿ç”¨é›¶å‘é‡æœ€å°åŒ–è·ç¦»å·®å¼‚
                image_vec = [0.0] * self.image_dim
            
            # 3. èåˆæŸ¥è¯¢å‘é‡
            query_embedding = text_vec + image_vec
            
            # æ„å»ºè¿‡æ»¤æ¡ä»¶ï¼ˆChromaDBè¦æ±‚å¤šæ¡ä»¶ä½¿ç”¨$andæ“ä½œç¬¦ï¼‰
            if category_filter and user_id:
                where_filter = {
                    "$and": [
                        {"user_id": str(user_id)},
                        {"category": category_filter}
                    ]
                }
            elif user_id:
                where_filter = {"user_id": str(user_id)}
            else:
                where_filter = None
            
            # å‘é‡æ£€ç´¢ï¼ˆå¤šå–2å€ï¼Œç”¨äºåç»­è¿‡æ»¤å’Œé‡æ’åºï¼‰
            results = self.wardrobe_collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k * 3,  # å¢åŠ å€™é€‰é›†
                where=where_filter,
                include=["metadatas", "distances"]  # è¿”å›å…ƒæ•°æ®å’Œè·ç¦»
            )
            
            if not results["ids"] or not results["ids"][0]:
                return []
            
            # æå–ç»“æœ
            item_ids = [int(id_str) for id_str in results["ids"][0]]
            metadatas = results["metadatas"][0]
            distances = results["distances"][0] if "distances" in results else [0] * len(item_ids)
            
            # æ··åˆè¿‡æ»¤ + é‡æ’åº
            scored_items = []
            for i, item_id in enumerate(item_ids):
                metadata = metadatas[i]
                distance = distances[i]
                
                # 1. è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†ï¼ˆè·ç¦»è½¬ç›¸ä¼¼åº¦: 1 - normalized_distanceï¼‰
                similarity_score = 1.0 - min(distance / 2.0, 1.0)  # å‡è®¾æ¬§æ°è·ç¦»
                
                # 2. å­£èŠ‚è¿‡æ»¤
                if season_filter:
                    item_seasons = metadata.get("season", "").split("/")
                    if not any(s in season_filter for s in item_seasons):
                        continue
                
                # 3. é¢œè‰²è¿‡æ»¤ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
                if color_filter:
                    item_color = metadata.get("color_en", "").lower()
                    if color_filter.lower() not in item_color:
                        continue
                    else:
                        similarity_score += 0.1  # é¢œè‰²åŒ¹é…åŠ åˆ†
                
                # 4. æè´¨è¿‡æ»¤ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
                if material_filter:
                    item_material = metadata.get("material_en", "").lower()
                    if material_filter.lower() not in item_material:
                        continue
                    else:
                        similarity_score += 0.05  # æè´¨åŒ¹é…åŠ åˆ†
                
                # 5. ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
                if similarity_score < min_score:
                    continue
                
                scored_items.append((item_id, similarity_score))
            
            # æŒ‰å¾—åˆ†é™åºæ’åˆ—
            scored_items.sort(key=lambda x: x[1], reverse=True)
            
            # è¿”å›Top-K
            final_ids = [item_id for item_id, _ in scored_items[:top_k]]
            
            print(f"ğŸ” æ··åˆæ£€ç´¢: å€™é€‰{len(item_ids)}ä»¶ â†’ è¿‡æ»¤{len(scored_items)}ä»¶ â†’ è¿”å›{len(final_ids)}ä»¶")
            return final_ids
            
        except Exception as e:
            print(f"âŒ å‘é‡æ£€ç´¢å¤±è´¥: error={e}")
            return []
    
    def batch_add_items(self, items: List[Dict[str, Any]]) -> Dict[str, int]:
        """
        æ‰¹é‡æ·»åŠ è¡£ç‰©å‘é‡ï¼ˆç”¨äºæ•°æ®è¿ç§»ï¼‰
        
        Args:
            items: è¡£ç‰©åˆ—è¡¨ï¼Œæ¯ä¸ªå¿…é¡»åŒ…å« 'id' å­—æ®µ
        
        Returns:
            {"success": æˆåŠŸæ•°é‡, "failed": å¤±è´¥æ•°é‡}
        """
        success_count = 0
        failed_count = 0
        
        for item in items:
            item_id = item.get("id")
            if not item_id:
                print(f"âš ï¸  è·³è¿‡: è¡£ç‰©ç¼ºå°‘idå­—æ®µ")
                failed_count += 1
                continue
            
            if self.add_item(item_id, item):
                success_count += 1
            else:
                failed_count += 1
        
        print(f"\n{'='*60}")
        print(f"æ‰¹é‡å‘é‡åŒ–å®Œæˆ: æˆåŠŸ={success_count}, å¤±è´¥={failed_count}")
        print(f"{'='*60}\n")
        
        return {"success": success_count, "failed": failed_count}


# å…¨å±€å•ä¾‹
_embedding_service: Optional[EmbeddingService] = None


def get_embedding_service() -> EmbeddingService:
    """è·å–å‘é‡åŒ–æœåŠ¡å•ä¾‹"""
    global _embedding_service
    if _embedding_service is None:
        _embedding_service = EmbeddingService()
    return _embedding_service
